{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndriyF-Git/Deep_Learning_IPZ43/blob/main/%D0%9A%D0%BE%D0%BF%D1%96%D1%8F_%D0%B7%D0%B0%D0%BF%D0%B8%D1%81%D0%BD%D0%B8%D0%BA%D0%B0_%22DeepLearn_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bE8bmSEeDLLT"
      },
      "outputs": [],
      "source": [
        "# !pip -q install -U kaggle\n",
        "# !kaggle --version\n",
        "\n",
        "# import os\n",
        "# os.environ[\"KAGGLE_API_TOKEN\"] = \"\"\n",
        "\n",
        "# # тест\n",
        "# !kaggle competitions list | head\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%bash\n",
        "# COMP=\"image-classification-real-or-ai-generated-photo\"\n",
        "# mkdir -p /content/$COMP\n",
        "# kaggle competitions download -c $COMP -p /content/$COMP\n",
        "# unzip -q /content/$COMP/*.zip -d /content/$COMP\n",
        "# ls -lah /content/$COMP\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FuzHL4_cHsmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install -U \"numpy==2.0.2\" \"opencv-python-headless==4.12.0.88\" timm scikit-learn \"albumentations>=1.4.20\""
      ],
      "metadata": {
        "id": "NiYJGzJ3Jmoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_df[img_col].head(20).tolist())\n",
        "# print(train_df[img_col].dtype)\n"
      ],
      "metadata": {
        "id": "cL4MeKaAOx47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # подивитись 10 випадкових значень\n",
        "# print(train_df[img_col].sample(10, random_state=42).tolist())\n"
      ],
      "metadata": {
        "id": "gNdvmanZO5V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "SEED = 42\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n",
        "WD = 1e-2\n",
        "MODEL_NAME = \"tf_efficientnetv2_s\"   # спробуй потім: \"efficientnetv2_s\"\n",
        "NUM_WORKERS = 0\n",
        "FOLDS_TO_TRAIN = 3  # для старту 2, потім постав 5\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_everything(SEED)\n"
      ],
      "metadata": {
        "id": "GQR4u9RqKBlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/g2drive')"
      ],
      "metadata": {
        "id": "2YbUhErS8u6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/image-classification-real-or-ai-generated-photo\"\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
        "TEST_DIR  = os.path.join(DATA_DIR, \"test\")\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
        "test_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
        "sub_df   = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
        "\n",
        "# submission columns\n",
        "id_col = sub_df.columns[0]\n",
        "target_col = sub_df.columns[1]\n",
        "\n",
        "# label col: беремо ту, якої нема в test.csv\n",
        "candidate_labels = [c for c in train_df.columns if c not in test_df.columns]\n",
        "if target_col in train_df.columns:\n",
        "    label_col = target_col\n",
        "elif len(candidate_labels) == 1:\n",
        "    label_col = candidate_labels[0]\n",
        "else:\n",
        "    for c in [\"label\", \"target\", \"class\", \"is_ai\", \"generated\"]:\n",
        "        if c in train_df.columns:\n",
        "            label_col = c\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(f\"Can't infer label column. train cols: {train_df.columns.tolist()}\")\n",
        "\n",
        "# image col: спільна колонка, що не id і не label\n",
        "img_col = None\n",
        "for c in train_df.columns:\n",
        "    if c != label_col and c in test_df.columns and c != id_col:\n",
        "        img_col = c\n",
        "        break\n",
        "if img_col is None:\n",
        "    img_col = [c for c in train_df.columns if c != label_col][0]\n",
        "\n",
        "print(\"id_col:\", id_col)\n",
        "print(\"img_col:\", img_col)\n",
        "print(\"label_col:\", label_col)\n",
        "print(\"submit target_col:\", target_col)\n",
        "\n",
        "import glob\n",
        "import os\n",
        "\n",
        "EXTS = [\".jpg\", \".jpeg\", \".png\", \".webp\"]\n",
        "\n",
        "def resolve_img_path(p, is_train: bool):\n",
        "    base = TRAIN_DIR if is_train else TEST_DIR\n",
        "    p = str(p)\n",
        "\n",
        "    # 1) якщо це вже існуючий шлях\n",
        "    if os.path.exists(p):\n",
        "        return p\n",
        "\n",
        "    # 2) якщо CSV містить \"train/...\" або \"test/...\"\n",
        "    cand = os.path.join(DATA_DIR, p)\n",
        "    if os.path.exists(cand):\n",
        "        return cand\n",
        "\n",
        "    # 3) пробуємо як \"base + p\" (якщо p вже з розширенням/папками)\n",
        "    cand = os.path.join(base, p)\n",
        "    if os.path.exists(cand):\n",
        "        return cand\n",
        "\n",
        "    # 4) якщо p без розширення — пробуємо додати розширення\n",
        "    root, ext = os.path.splitext(p)\n",
        "    if ext == \"\":\n",
        "        for e in EXTS:\n",
        "            cand = os.path.join(base, p + e)\n",
        "            if os.path.exists(cand):\n",
        "                return cand\n",
        "\n",
        "        # 5) якщо файли у підпапках — шукаємо рекурсивно\n",
        "        # (обережно: glob трохи повільніший, але як fallback — норм)\n",
        "        for e in EXTS:\n",
        "            hits = glob.glob(os.path.join(base, \"**\", p + e), recursive=True)\n",
        "            if hits:\n",
        "                return hits[0]\n",
        "\n",
        "    # 6) останній шанс: шукаємо за basename як є (коли p = \"306.jpg\", але лежить в підпапці)\n",
        "    hits = glob.glob(os.path.join(base, \"**\", os.path.basename(p)), recursive=True)\n",
        "    if hits:\n",
        "        return hits[0]\n",
        "\n",
        "    return os.path.join(base, p)  # повертаємо “очікуваний” шлях для нормальної помилки\n",
        "\n",
        "\n",
        "# sanity check\n",
        "print(\"train sample path:\", resolve_img_path(train_df.loc[0, img_col], True))\n",
        "print(\"test sample path:\", resolve_img_path(test_df.loc[0, img_col], False))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "av-C3KC6KYIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfms = A.Compose([\n",
        "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
        "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "\n",
        "    A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.75, 1.0), ratio=(0.9, 1.1)),\n",
        "\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ColorJitter(p=0.5),\n",
        "    A.GaussianBlur(blur_limit=(3, 7), p=0.1),\n",
        "    # A.ImageCompression(quality_lower=60, quality_upper=100, p=0.35),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "valid_tfms = A.Compose([\n",
        "    A.LongestMaxSize(max_size=IMG_SIZE),\n",
        "    A.PadIfNeeded(min_height=IMG_SIZE, min_width=IMG_SIZE, border_mode=cv2.BORDER_REFLECT_101),\n",
        "\n",
        "    A.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n",
        "\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "TRJ4Njs8KZGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, df, is_train=True, tfms=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.is_train = is_train\n",
        "        self.tfms = tfms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        path = resolve_img_path(row[img_col], self.is_train)\n",
        "\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Can't read image: {path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.tfms is not None:\n",
        "            img = self.tfms(image=img)[\"image\"]\n",
        "\n",
        "        if self.is_train:\n",
        "            y = float(row[label_col])\n",
        "            y = torch.tensor([y], dtype=torch.float32)\n",
        "            return img, y\n",
        "        else:\n",
        "            return img, row[id_col]\n"
      ],
      "metadata": {
        "id": "wX2OhGHxLooF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "train_df[\"fold\"] = -1\n",
        "for fold, (_, val_idx) in enumerate(skf.split(train_df, train_df[label_col].astype(int))):\n",
        "    train_df.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "train_df[\"fold\"].value_counts().sort_index()\n"
      ],
      "metadata": {
        "id": "_Ulc3Zf8LrPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    return timm.create_model(MODEL_NAME, pretrained=True, num_classes=1)\n",
        "\n",
        "def train_fold(fold=0, save_best_by=\"loss\"):\n",
        "    \"\"\"\n",
        "    save_best_by: \"loss\" (min valid_loss) або \"auc\" (max valid_auc)\n",
        "    \"\"\"\n",
        "    assert save_best_by in [\"loss\", \"auc\"]\n",
        "\n",
        "    tr = train_df[train_df.fold != fold].copy()\n",
        "    va = train_df[train_df.fold == fold].copy()\n",
        "\n",
        "    dl_tr = DataLoader(\n",
        "        ImgDataset(tr, True, train_tfms),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=False\n",
        "    )\n",
        "    dl_va = DataLoader(\n",
        "        ImgDataset(va, True, valid_tfms),\n",
        "        batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=False\n",
        "    )\n",
        "\n",
        "    model = build_model().to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "    scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "    # best tracking\n",
        "    best_loss = 1e18\n",
        "    best_auc = -1e18\n",
        "    best_path = f\"/content/best_fold{fold}_{save_best_by}.pt\"\n",
        "\n",
        "    history = {\n",
        "        \"fold\": [],\n",
        "        \"epoch\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"valid_loss\": [],\n",
        "        \"valid_acc\": [],\n",
        "        \"valid_auc\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        # -------- train --------\n",
        "        model.train()\n",
        "        tr_losses = []\n",
        "\n",
        "        for x, y in tqdm(dl_tr, desc=f\"fold{fold} ep{ep+1}/{EPOCHS} train\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            with autocast(enabled=use_amp):\n",
        "                logits = model(x)\n",
        "                loss = loss_fn(logits, y)\n",
        "\n",
        "            if use_amp:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "            tr_losses.append(loss.item())\n",
        "\n",
        "        # -------- valid --------\n",
        "        model.eval()\n",
        "        va_losses = []\n",
        "        probs_all = []\n",
        "        targets_all = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(dl_va, desc=f\"fold{fold} ep{ep+1}/{EPOCHS} valid\"):\n",
        "                x, y = x.to(device), y.to(device)\n",
        "\n",
        "                with autocast(enabled=use_amp):\n",
        "                    logits = model(x)\n",
        "                    loss = loss_fn(logits, y)\n",
        "\n",
        "                va_losses.append(loss.item())\n",
        "\n",
        "                probs = torch.sigmoid(logits).detach().cpu().numpy().reshape(-1)\n",
        "                targets = y.detach().cpu().numpy().reshape(-1)\n",
        "\n",
        "                probs_all.append(probs)\n",
        "                targets_all.append(targets)\n",
        "\n",
        "        tr_loss = float(np.mean(tr_losses))\n",
        "        va_loss = float(np.mean(va_losses))\n",
        "\n",
        "        probs_all = np.concatenate(probs_all)\n",
        "        targets_all = np.concatenate(targets_all).astype(int)\n",
        "\n",
        "        preds = (probs_all >= 0.5).astype(int)\n",
        "        va_acc = float((preds == targets_all).mean())\n",
        "\n",
        "        # AUC може впасти лише якщо раптом один клас у fold (у тебе 50/50, тож має бути ок)\n",
        "        try:\n",
        "            va_auc = float(roc_auc_score(targets_all, probs_all))\n",
        "        except ValueError:\n",
        "            va_auc = float(\"nan\")\n",
        "\n",
        "        current_lr = opt.param_groups[0][\"lr\"]\n",
        "\n",
        "        history[\"fold\"].append(fold)\n",
        "        history[\"epoch\"].append(ep + 1)\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"valid_loss\"].append(va_loss)\n",
        "        history[\"valid_acc\"].append(va_acc)\n",
        "        history[\"valid_auc\"].append(va_auc)\n",
        "        history[\"lr\"].append(current_lr)\n",
        "\n",
        "        print(\n",
        "            f\"fold{fold} ep{ep+1}: \"\n",
        "            f\"train_loss={tr_loss:.4f} valid_loss={va_loss:.4f} \"\n",
        "            f\"valid_acc={va_acc:.4f} valid_auc={va_auc:.4f} lr={current_lr:.2e}\"\n",
        "        )\n",
        "\n",
        "        # save best\n",
        "        if save_best_by == \"loss\":\n",
        "            if va_loss < best_loss:\n",
        "                best_loss = va_loss\n",
        "                torch.save(model.state_dict(), best_path)\n",
        "        else:  # \"auc\"\n",
        "            if (not np.isnan(va_auc)) and (va_auc > best_auc):\n",
        "                best_auc = va_auc\n",
        "                torch.save(model.state_dict(), best_path)\n",
        "\n",
        "        sched.step()\n",
        "\n",
        "    return best_path, pd.DataFrame(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BpLT90E3LvMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_paths = []\n",
        "metrics_dfs = []\n",
        "\n",
        "for fold in range(FOLDS_TO_TRAIN):\n",
        "    best_path, df_hist = train_fold(fold, save_best_by=\"loss\")\n",
        "    model_paths.append(best_path)\n",
        "    metrics_dfs.append(df_hist)\n",
        "\n",
        "metrics = pd.concat(metrics_dfs, ignore_index=True)\n",
        "metrics.to_csv(\"/content/metrics_history.csv\", index=False)\n",
        "print(\"Saved metrics:\", \"/content/metrics_history.csv\")\n",
        "model_paths\n"
      ],
      "metadata": {
        "id": "tQizTH5qLyR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_test(model_paths):\n",
        "    dl_te = DataLoader(\n",
        "        ImgDataset(test_df, False, valid_tfms),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False\n",
        "    )\n",
        "\n",
        "    all_ids = None\n",
        "    all_preds = None\n",
        "    use_amp = (device.type == \"cuda\")\n",
        "\n",
        "    for mp in model_paths:\n",
        "        model = build_model().to(device)\n",
        "        model.load_state_dict(torch.load(mp, map_location=device))\n",
        "        model.eval()\n",
        "\n",
        "        preds = []\n",
        "        ids = []\n",
        "        for x, img_id in tqdm(dl_te, desc=f\"infer {os.path.basename(mp)}\"):\n",
        "            x = x.to(device)\n",
        "            with autocast(enabled=use_amp):\n",
        "              logits = model(x)\n",
        "            logits = logits.float().cpu().numpy().reshape(-1)\n",
        "            prob = 1 / (1 + np.exp(-logits))\n",
        "            preds.append(prob)\n",
        "            ids.extend(list(img_id))\n",
        "\n",
        "        preds = np.concatenate(preds)\n",
        "        if all_preds is None:\n",
        "            all_preds = preds\n",
        "            all_ids = ids\n",
        "        else:\n",
        "            all_preds += preds\n",
        "\n",
        "    all_preds /= len(model_paths)\n",
        "    return pd.DataFrame({id_col: all_ids, target_col: all_preds})\n",
        "\n",
        "pred_df = predict_test(model_paths)\n",
        "\n",
        "submission = sub_df[[id_col, target_col]].merge(pred_df, on=id_col, how=\"left\", suffixes=(\"\", \"_pred\"))\n",
        "submission[target_col] = submission[f\"{target_col}_pred\"].fillna(0.5)\n",
        "submission = submission[[id_col, target_col]]\n",
        "\n",
        "out_path = \"/content/submission.csv\"\n",
        "submission.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"Saved:\", out_path)\n",
        "submission.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kzYJDVgpLzrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg = metrics.groupby(\"epoch\")[[\"train_loss\",\"valid_loss\",\"valid_acc\",\"valid_auc\"]].mean().reset_index()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(avg[\"epoch\"], avg[\"train_loss\"], label=\"train_loss_mean\")\n",
        "plt.plot(avg[\"epoch\"], avg[\"valid_loss\"], label=\"valid_loss_mean\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Mean Loss across folds\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(avg[\"epoch\"], avg[\"valid_acc\"], label=\"valid_acc_mean\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Mean Validation Accuracy across folds\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(avg[\"epoch\"], avg[\"valid_auc\"], label=\"valid_auc_mean\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"AUC\"); plt.title(\"Mean Validation AUC across folds\"); plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "eb-XW6hyrJz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"/content/g2drive/MyDrive/kaggle_real_vs_ai\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "metrics.to_csv(f\"{SAVE_DIR}/metrics_history.csv\", index=False)\n",
        "best_by_fold.to_csv(f\"{SAVE_DIR}/best_by_fold.csv\", index=False)\n",
        "\n",
        "# якщо хочеш зберегти моделі:\n",
        "for p in model_paths:\n",
        "    !cp -f \"{p}\" \"{SAVE_DIR}/\"\n",
        "print(\"Saved to:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "O_s2lYN5-fUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from IPython.display import display\n",
        "\n",
        "best_by_fold = (\n",
        "    metrics.sort_values([\"fold\", \"valid_loss\"])\n",
        "           .groupby(\"fold\", as_index=False)\n",
        "           .first()\n",
        "           .loc[:, [\"fold\",\"epoch\",\"train_loss\",\"valid_loss\",\"valid_acc\",\"valid_auc\",\"lr\"]]\n",
        ")\n",
        "\n",
        "display(\n",
        "    best_by_fold.style.format({\n",
        "        \"train_loss\":\"{:.4f}\",\n",
        "        \"valid_loss\":\"{:.4f}\",\n",
        "        \"valid_acc\":\"{:.2%}\",\n",
        "        \"valid_auc\":\"{:.4f}\",\n",
        "        \"lr\":\"{:.2e}\"\n",
        "    })\n",
        ")\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"valid_loss_mean±std\": [f\"{best_by_fold['valid_loss'].mean():.4f} ± {best_by_fold['valid_loss'].std(ddof=1):.4f}\"],\n",
        "    \"valid_acc_mean±std\":  [f\"{best_by_fold['valid_acc'].mean():.2%} ± {best_by_fold['valid_acc'].std(ddof=1):.2%}\"],\n",
        "    \"valid_auc_mean±std\":  [f\"{best_by_fold['valid_auc'].mean():.4f} ± {best_by_fold['valid_auc'].std(ddof=1):.4f}\"],\n",
        "})\n",
        "display(summary)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rP35zACxoJBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, time\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/real_vs_ai_saved\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# збережемо чекпойнти\n",
        "for p in model_paths:\n",
        "    dst = os.path.join(SAVE_DIR, os.path.basename(p))\n",
        "    shutil.copy2(p, dst)\n",
        "    print(\"Saved:\", dst)\n",
        "\n",
        "# збережемо метрики, якщо є\n",
        "if \"metrics\" in globals():\n",
        "    metrics_path = os.path.join(SAVE_DIR, \"metrics_history.csv\")\n",
        "    metrics.to_csv(metrics_path, index=False)\n",
        "    print(\"Saved:\", metrics_path)\n",
        "\n",
        "# збережемо графіки, якщо ти їх вже робив як loss_curve.png і т.д.\n",
        "for fn in [\"loss_curve.png\", \"accuracy_curve.png\", \"auc_curve.png\"]:\n",
        "    src = f\"/content/{fn}\"\n",
        "    if os.path.exists(src):\n",
        "        dst = os.path.join(SAVE_DIR, fn)\n",
        "        shutil.copy2(src, dst)\n",
        "        print(\"Saved:\", dst)\n",
        "\n",
        "print(\"\\nAll saved to:\", SAVE_DIR)\n"
      ],
      "metadata": {
        "id": "NeqXHvmZ6LyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[label_col].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "UrSdyqr4F3Bb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}